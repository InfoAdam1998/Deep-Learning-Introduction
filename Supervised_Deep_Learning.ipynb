{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network Terminologies\n",
        "\n",
        "**Neuron**:\n",
        "A neuron in a neural network typically performs two main tasks. First, it calculates the weighted sum of its inputs (which are the outputs of other neurons or features) along with a bias term. Then, it applies an activation function to this sum to determine the neuron’s output. The activation function introduces non-linearity, which helps the network learn complex patterns.\n",
        "\n",
        "**Activation functions**\n",
        "\n",
        "* The Threshold Function is historically used in early models like perceptrons, which are binary classification models. However, it is rarely used in modern neural networks because it lacks smooth gradients, making it unsuitable for backpropagation.\n",
        "\n",
        "* The Sigmoid Function is primarily used in binary classification tasks, especially at the output layer of networks, to produce probabilities. It’s also used in some layers of older neural networks and certain types of models like logistic regression.\n",
        "\n",
        "* The Rectifier (ReLU) is most commonly found in the hidden layers of deep learning models, particularly in convolutional neural networks (CNNs) and fully connected networks. Its efficiency and ability to mitigate vanishing gradient issues make it a go-to activation function in most deep learning applications.\n",
        "\n",
        "* The Hyperbolic Tangent Function (tanh) is frequently used in the hidden layers of recurrent neural networks (RNNs) and other models where it’s beneficial for the outputs to be centered around zero. Its centered output helps improve the convergence of certain types of models.\n",
        "\n",
        "**Neurons learn to focus on specific inputs by adjusting their weights through training, emphasizing relevant inputs and downplaying irrelevant ones.**"
      ],
      "metadata": {
        "id": "UeRmDewphqfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How do artificial neural networks work?\n",
        "\n",
        "* Input Layer: The network starts with an input layer where each neuron represents a feature from the dataset (e.g., in image classification, each pixel would be an input neuron). The input data is fed into the network.\n",
        "\n",
        "* Weighted Sum: Each neuron in the next layer receives inputs from neurons in the previous layer. These inputs are multiplied by weights (which represent the importance of the connection) and then summed up. Additionally, a bias term is added to shift the result.\n",
        "\n",
        "* Activation Function: After calculating the weighted sum, the neuron applies an activation function (like ReLU, sigmoid, or tanh). This function determines whether the neuron should be \"activated\" (i.e., pass a signal forward). It introduces non-linearity, allowing the network to learn more complex patterns.\n",
        "\n",
        "* Hidden Layers: The output from the activation function of one layer becomes the input for the next layer. There can be multiple hidden layers in a neural network, where each layer processes the data further, identifying more abstract patterns as the information flows through the network.\n",
        "\n",
        "* Output Layer: The final layer, called the output layer, produces the final predictions or classifications. For example, in a binary classification task, the output could be a probability score between 0 and 1.\n",
        "\n",
        "* Loss Function: After the network produces an output, it compares this output to the true target value using a **loss function**. The loss function measures how far off the network's predictions are from the correct answers.\n",
        "\n",
        "* Backpropagation: To improve accuracy, the network adjusts its internal weights. This is done through a process called backpropagation, where the error from the output is propagated back through the network. The weights are updated using gradient descent (an optimization algorithm), where small adjustments are made to reduce the error.\n",
        "\n",
        "* Training: The neural network is trained over many epochs (iterations) where it sees the training data repeatedly. Over time, the network learns to adjust its weights to **minimize the loss function** and improve prediction accuracy.\n",
        "\n",
        "* Prediction: After training, the network can take in new, unseen data, pass it through the layers, and make predictions based on what it has learned."
      ],
      "metadata": {
        "id": "GDgu39fUnCAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN Optimization Flows\n",
        "\n",
        "**Note**\n",
        "\n",
        "Before training, weights are randomly assigned and uniformly affect all inputs.\n",
        "After training, weights are adjusted to reflect the significance of specific inputs for each neuron, leading to a more effective and specialized neural network.\n",
        "\n",
        "* **Gradient Descent**:\n",
        "Gradient descent is an optimization algorithm used to minimize the error or loss of a model. It works by adjusting the model’s parameters (like weights in a neural network) step by step to reduce the difference between the model’s predictions and the actual targets.\n",
        "\n",
        "*  **Stochastic Gradient Descent** (SGD):\n",
        "Stochastic Gradient Descent is a variation of gradient descent. Instead of using the entire dataset to compute the gradient and update the weights, it uses only one data point (or a small batch of data) at each step. Think of it like walking down the hill, but instead of carefully calculating the steepest direction based on all the terrain, you make a guess based on just a small part of it.\n",
        "This method is much faster for large datasets because it updates the model more frequently, but it introduces some noise in the updates, so it might bounce around a bit more before reaching the bottom.\n",
        "\n",
        "Gradient can be deterministic while stchastic is considered random\n",
        "\n",
        "* **Mini-Batch Gradient Descent** algorithm is a compromise between Batch Gradient Descent and Stochastic Gradient Descent (SGD). It processes a small, random subset of the training data (called a mini-batch) to update the model's weights in each iteration.\n",
        "\n",
        "* **Backpropagation**:\n",
        "Backpropagation is the technique used to calculate the gradients of the error with respect to each weight in the network, so they can be updated during gradient descent. In a neural network, the input is passed through the layers to produce an output (this is called forward propagation). The output is compared to the target, and the error is calculated using a loss function.\n",
        "Backpropagation works by sending the error backwards through the network, layer by layer, to determine how much each weight contributed to the error.\n",
        "Once the contribution of each weight to the error is known, gradient descent (or stochastic gradient descent) can be used to update the weights and reduce the error."
      ],
      "metadata": {
        "id": "wCKkP04Hrzn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Artificial neural network\n",
        "\n",
        "Number of neurons are determined by input features"
      ],
      "metadata": {
        "id": "Li3XtozEOrv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import TensorFlow and Keras libraries\n",
        "# TensorFlow is the backend framework, and Keras provides the API for building neural networks.\n",
        "from tensorflow import tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "tf.__version__  # Check TensorFlow version\n",
        "\n",
        "# Step 2: Initialize an Artificial Neural Network (ANN)\n",
        "# This creates a sequential model where layers are added one after another.\n",
        "ann = tf.keras.Sequential()\n",
        "\n",
        "# Step 3: Adding the first hidden layer with 6 neurons and ReLU activation\n",
        "# This layer will process input features and apply the ReLU activation function.\n",
        "ann.add(tf.keras.layers.Dense(units = 6, activation = \"relu\"))\n",
        "\n",
        "# Step 4: Adding the second hidden layer with 6 neurons and ReLU activation\n",
        "# Another hidden layer is added to increase the network's capacity to learn.\n",
        "ann.add(tf.keras.layers.Dense(units = 6, activation = \"relu\"))\n",
        "\n",
        "# Step 5: Adding the output layer with 1 neuron and Sigmoid activation\n",
        "# The output layer uses the Sigmoid activation for binary classification.\n",
        "ann.add(tf.keras.layers.Dense(units = 1, activation = \"sigmoid\"))\n",
        "\n",
        "# Step 6: Compiling the ANN with Adam optimizer and binary crossentropy loss\n",
        "# The Adam optimizer is used for efficient optimization, and binary crossentropy is used for classification.\n",
        "ann.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "# Step 7: Training the ANN on the training set\n",
        "# The network is trained with a batch size of 32 over 100 epochs.\n",
        "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)\n",
        "\n",
        "# Step 8: Predicting the test set results\n",
        "# This generates predictions for the test set, and values above 0.5 are classified as 1 (True).\n",
        "y_pred = ann.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Step 9: Making a new prediction for a specific input\n",
        "# The input values are transformed before prediction, and the result is compared to 0.5 for classification.\n",
        "new_prediction = ann.predict(sc.transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
        "new_prediction = (new_prediction > 0.5)"
      ],
      "metadata": {
        "id": "jT5FqWD8h-YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating ANN"
      ],
      "metadata": {
        "id": "FF0NxBgzf2aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries for evaluating the ANN\n",
        "# KerasClassifier wraps the Keras model for compatibility with scikit-learn.\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Step 2: Define a function to build the ANN classifier\n",
        "# This function initializes the ANN architecture and compiles it for training.\n",
        "def build_classifier():\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "\n",
        "# Step 3: Create a KerasClassifier instance for cross-validation\n",
        "# This wraps the ANN model, allowing it to be used with scikit-learn's cross-validation tools.\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)\n",
        "\n",
        "# Step 4: Perform cross-validation to evaluate the ANN\n",
        "# This calculates the accuracy of the model using 10-fold cross-validation.\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
        "\n",
        "# Step 5: Calculate mean and variance of accuracies\n",
        "# The mean gives the average accuracy, and variance indicates the model's stability.\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "\n",
        "# Step 6: Comment on potential improvement using dropout regularization\n",
        "# Dropout can help reduce overfitting by randomly setting a fraction of input units to 0 during training."
      ],
      "metadata": {
        "id": "XW0NESPNf6c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning ANN"
      ],
      "metadata": {
        "id": "IYdRKALypVGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries for hyperparameter tuning\n",
        "# GridSearchCV is used to find the best hyperparameters by testing all combinations.\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Step 2: Define a function to build the ANN classifier with an optimizer parameter\n",
        "# This allows the optimizer to be varied during the grid search.\n",
        "def build_classifier(optimizer):\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
        "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return classifier\n",
        "\n",
        "# Step 3: Create a KerasClassifier instance for grid search\n",
        "# This prepares the model for hyperparameter tuning using GridSearchCV.\n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "\n",
        "# Step 4: Define a parameter grid for tuning the model\n",
        "# This specifies which hyperparameters and values to explore during tuning.\n",
        "parameters = {'batch_size': [25, 32],\n",
        "              'epochs': [100, 500],\n",
        "              'optimizer': ['adam', 'rmsprop']}\n",
        "\n",
        "# Step 11: Set up GridSearchCV to find the best parameters\n",
        "# This runs the grid search across the specified hyperparameter combinations.\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10)\n",
        "\n",
        "# Step 5: Fit the grid search on the training data\n",
        "# This trains the model for each combination of parameters specified in the grid.\n",
        "grid_search = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Retrieve the best parameters and best accuracy from grid search\n",
        "# This allows you to see the most effective hyperparameters found during the tuning process.\n",
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_"
      ],
      "metadata": {
        "id": "zwIXNqdCpDq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional neural network"
      ],
      "metadata": {
        "id": "pa2TZfqOcTdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Convolutional Neural Network (CNN) is a type of deep learning model primarily used for processing structured grid data, such as images. CNNs are designed to automatically and adaptively learn spatial hierarchies of features through backpropagation, making them highly effective for image recognition, object detection, and similar tasks. They utilize convolutional layers to scan input images with filters (kernels) to capture patterns and features at various spatial resolutions.\n",
        "\n",
        "* The **convolution operation** involves sliding a filter (or kernel) over the input image to create a feature map, allowing the model to detect patterns such as edges and textures by calculating the dot product between the filter and local regions of the input.\n",
        "\n",
        "* After the convolution, the **ReLU** (Rectified Linear Unit) step comes in. ReLU is a function that makes all negative values in the feature map zero, while keeping the positive values unchanged. This introduces non-linearity, meaning the network can now learn more complex patterns beyond just simple linear patterns.\n",
        "\n",
        "**Why introduce non-linearity?** In a **linear situation**, it's like the model can only capture the basic shadows or outlines of things. So, if you have a black-and-white photo, a linear model might detect where the dark and light areas are (the shadows and highlights), but it won't understand more complex details like shapes or textures.\n",
        "\n",
        "* **Pooling layers** then reduce the size of the feature maps by downsampling, helping to preserve important features while making the model more efficient. Max pooling, for instance, keeps only the maximum value in each patch, while average pooling computes the average value.\n",
        "\n",
        "* The **flattening** step converts these pooled maps into a one-dimensional vector, preparing the data for the fully connected layers.\n",
        "\n",
        "* **Fully connected layers** connect each neuron to every neuron in the previous layer, allowing the model to learn more global patterns based on the extracted features.\n",
        "\n",
        "* The **softmax function** is applied in the final output layer for multi-class classification, converting raw output scores into probabilities that sum to one. This enables the model to predict which class has the highest probability.\n",
        "\n",
        "* Finally, **cross-entropy loss** is used to measure the difference between the predicted probabilities and the actual labels, guiding the model's optimization process during training. It helps adjust the weights to minimize errors and improve accuracy."
      ],
      "metadata": {
        "id": "OKzlaiIccgSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional notes**\n",
        "* A black and white image is represented by a 2D array, where each pixel has a single intensity value (e.g., grayscale), while a color image is a 3D array, where each pixel has three intensity values (e.g., Red, Green, and Blue channels).\n",
        "\n",
        "* Stride refers to how much the filter moves or shifts across the input image during the convolution operation. A stride of 1 means the filter moves one pixel at a time, while a higher stride skips more pixels, reducing the size of the output feature map.\n",
        "\n",
        "* A filter (kernel) is a small matrix (like 3x3) used to scan across the image, performing element-wise multiplication to detect specific patterns such as edges, textures, or colors.\n",
        "\n",
        "* A feature map is the output generated after applying the filter to an image. It highlights specific patterns or features like edges in different parts of the image.\n",
        "\n",
        "* Applying filters during convolution reduces the dimensionality of the image, making processing more efficient by extracting the most important features while discarding unnecessary information.\n",
        "\n",
        "* Feature maps are the outputs of filters applied to an image, and each filter is designed to capture a unique pattern (like detecting edges, corners, or textures). For example, one filter may detect horizontal edges, while another might detect vertical edges.\n",
        "* Filters are also adjusted during back propagation\n",
        "\n",
        "* Neurons in the final layer do the voting\n",
        "\n",
        "* Flattening: The flattening process takes all these pooled feature maps and combines them into a single long vector. For example, if you have three pooled feature maps of size 5x5, the flattening step will convert them into a vector of size 75 (3 * 5 * 5)."
      ],
      "metadata": {
        "id": "ZzpQXj82wXzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Data preprocessing"
      ],
      "metadata": {
        "id": "hBLMNjx4L9e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary library for image data augmentation\n",
        "# This library allows us to preprocess and augment image data to improve model generalization.\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Creating an instance of ImageDataGenerator for the training set\n",
        "# This object rescales pixel values and applies various augmentations to increase dataset diversity.\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,         # Normalizes pixel values to the range [0, 1]\n",
        "    shear_range=0.2,       # Applies shear transformations to the images\n",
        "    zoom_range=0.2,        # Applies random zoom transformations to the images\n",
        "    horizontal_flip=True    # Randomly flips images horizontally\n",
        ")\n",
        "\n",
        "# Generating the training set from the directory\n",
        "# This loads images from the specified directory, resizes them, and applies the augmentations.\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    'dataset/training_set',  # Directory containing training images\n",
        "    target_size=(64, 64),    # Resizes images to 64x64 pixels\n",
        "    batch_size=32,           # Number of images to be yielded from the generator per batch\n",
        "    class_mode='binary'      # Specifies that this is a binary classification problem\n",
        ")\n",
        "\n",
        "# Creating an instance of ImageDataGenerator for the test set\n",
        "# This object rescales pixel values only (no augmentation).\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Generating the test set from the directory\n",
        "# This loads images from the specified directory and resizes them without augmentation.\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    'dataset/test_set',      # Directory containing test images\n",
        "    target_size=(64, 64),    # Resizes images to 64x64 pixels\n",
        "    batch_size=32,           # Number of images to be yielded from the generator per batch\n",
        "    class_mode='binary'      # Specifies that this is a binary classification problem\n",
        ")\n",
        "\n",
        "# Fitting the CNN model to the training set\n",
        "# This trains the model using the training data and validates it with the test set.\n",
        "cnn.fit(\n",
        "    training_set,           # Training data generator\n",
        "    steps_per_epoch=8000,  # Number of batches to run for each epoch (depends on your data size)\n",
        "    epochs=25,              # Total number of epochs to train the model\n",
        "    validation_data=test_set,  # Validation data generator\n",
        "    validation_steps=2000   # Number of validation batches to run\n",
        ")"
      ],
      "metadata": {
        "id": "BOjR06QEcUE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries for building a CNN\n",
        "from keras.models import Sequential  # To create a sequential model\n",
        "from keras.layers import Conv2D  # For convolutional layers to extract features\n",
        "from keras.layers import MaxPooling2D  # For pooling layers to reduce dimensions\n",
        "from keras.layers import Flatten  # To convert 2D data to 1D\n",
        "from keras.layers import Dense  # For fully connected layers to make predictions\n",
        "\n",
        "# Initializing the CNN model\n",
        "cnn = Sequential()  # A sequential model to stack layers linearly.\n",
        "\n",
        "# Step 1 - Adding the first convolutional layer\n",
        "cnn.add(Conv2D(32, (3, 3),  # Applies 32 filters (or kernels) of size 3x3 to the input image.\n",
        "               input_shape=(64, 64, 3),  # The input shape is set to (64, 64, 3), meaning the images are 64x64 pixels with 3 color channels (RGB).\n",
        "               activation='relu'))  # The ReLU activation function introduces non-linearity, allowing the model to learn complex patterns.\n",
        "\n",
        "# Step 2 - Adding the first pooling layer\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling with a 2x2 pool size reduces the spatial dimensions of the feature maps.\n",
        "\n",
        "# Step 3 - Adding the second convolutional layer\n",
        "cnn.add(Conv2D(32, (3, 3),  # Applies another 32 filters of size 3x3 for additional feature extraction.\n",
        "               activation='relu'))  # ReLU activation function to maintain non-linearity in the model.\n",
        "\n",
        "# Step 4 - Adding a second pooling layer\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))  # Another max pooling layer to downsample the feature maps further.\n",
        "\n",
        "# Step 5 - Flattening the pooled feature maps\n",
        "cnn.add(Flatten())  # Converts the pooled 2D feature maps into a 1D vector to prepare for fully connected layers.\n",
        "\n",
        "# Step 6 - Adding fully connected layers\n",
        "cnn.add(Dense(units=128,  # A dense layer with 128 neurons to learn complex patterns from the features.\n",
        "              activation='relu'))  # ReLU activation for non-linearity.\n",
        "\n",
        "# Step 7 - Adding the output layer\n",
        "cnn.add(Dense(units=1,  # Output layer with 1 neuron for binary classification (cat or dog).\n",
        "              activation='sigmoid'))  # Sigmoid activation to output a probability between 0 and 1.\n",
        "\n",
        "# Step 8 - Compiling the CNN\n",
        "cnn.compile(optimizer='adam',  # Adam optimizer is used for efficient gradient descent.\n",
        "            loss='binary_crossentropy',  # Binary cross-entropy loss function for binary classification tasks.\n",
        "            metrics=['accuracy'])  # Accuracy metric to evaluate the model's performance during training."
      ],
      "metadata": {
        "id": "o8VDbrVt54pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Importing necessary libraries for building a CNN\n",
        "# This library provides functionalities to create and manage deep learning models.\n",
        "import numpy as np  # For numerical operations\n",
        "from keras.preprocessing import image  # For image loading and preprocessing\n",
        "\n",
        "# Step 2: Loading and preprocessing the test image\n",
        "# This step loads the image from the specified path and resizes it to match the model's input shape.\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg',\n",
        "                            target_size=(64, 64))  # Resize to 64x64 pixels.\n",
        "\n",
        "# Step 3: Converting the image to a numpy array\n",
        "# This converts the loaded image into a numpy array format for processing.\n",
        "test_image = image.img_to_array(test_image)  # Converts the image to a numpy array.\n",
        "\n",
        "# Step 4: Expanding dimensions to match the model's input shape\n",
        "# This expands the dimensions of the numpy array to create a batch of size 1, required for prediction.\n",
        "test_image = np.expand_dims(test_image,\n",
        "                            axis=0)  # Adds an additional dimension for batch size.\n",
        "\n",
        "# Step 5: Making a prediction on the processed test image\n",
        "# This uses the CNN model to predict the class of the preprocessed image, outputting a probability score.\n",
        "result = cnn.predict(test_image)  # Predicts the class for the test image.\n",
        "\n",
        "# Step 6: Getting the class indices for prediction mapping\n",
        "# This retrieves the mapping of class labels (like 'cat' and 'dog') to their corresponding indices used during training.\n",
        "class_indices = training_set.class_indices  # Get class indices for mapping.\n",
        "\n",
        "# Step 7: Interpreting the prediction\n",
        "# This checks the predicted probability; if greater than 0.5, classify as 'dog', otherwise classify as 'cat'.\n",
        "if result[0][0] > 0.5:  # Checks the predicted probability score.\n",
        "    prediction = 'dog'  # Classifies as 'dog' if probability > 0.5.\n",
        "else:\n",
        "    prediction = 'cat'  # Otherwise, classifies as 'cat'."
      ],
      "metadata": {
        "id": "swqAY7xW9mtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes**\n",
        "\n",
        "* Single Image Needs a Batch Dimension: When you're predicting with just one image, you still need to tell the model that this is a \"batch\" of size 1. Without this batch dimension, the model wouldn't know how to interpret the input.\n",
        "\n",
        "* If we have 8000 images and the batch is 32, with 25 epochs"
      ],
      "metadata": {
        "id": "ut0pf3gFVXy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent neural network"
      ],
      "metadata": {
        "id": "eIrHLPVR1cIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a Recurrent Neural Network**\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a type of neural network specifically designed to process sequential data like language, speech, or time-series data, where the order of inputs matters. Unlike traditional neural networks that process inputs independently, RNNs have a mechanism to retain information from previous inputs, giving them a form of short-term memory.\n",
        "\n",
        "**Key Components of an RNN**\n",
        "\n",
        "**Input Sequence:**\n",
        "\n",
        "* RNNs are designed to handle data that arrives in sequences, such as sentences or time-based signals (e.g., audio). Each element of the sequence (such as each word in a sentence) is processed one at a time, allowing the network to analyze the order of inputs.\n",
        "Hidden State (Memory):\n",
        "\n",
        "* The hidden state in an RNN functions as a memory. As each new input is received, the RNN updates this hidden state. This means that while processing a sentence, for instance, the RNN remembers the previous words by updating its hidden state with each new word.\n",
        "Loops in Architecture:\n",
        "\n",
        "* RNNs have loops in their structure that allow the hidden state to be passed from one time step to the next. This allows the network to carry information forward in time, making it possible for the RNN to generate output based on both the current input and everything it has seen so far.\n",
        "\n",
        "**How RNNs Work (Using a Language Example)**\n",
        "\n",
        "Let's say an RNN is processing the sentence \"The cat sat on the mat\":\n",
        "\n",
        "* Step 1: The first word, \"The,\" is input into the network. The RNN processes this word and updates its hidden state to remember \"The.\"\n",
        "\n",
        "* Step 2: The next word, \"cat,\" is input. The RNN uses its hidden state from the previous step (which contains information about \"The\") and combines it with the new input (\"cat\"). Now, the hidden state holds information about both \"The\" and \"cat.\"\n",
        "\n",
        "* Step 3: As the RNN continues through the sentence with each subsequent word (\"sat,\" \"on,\" etc.), it updates the hidden state, building context for the entire sentence. By the end, the RNN has a memory of the sentence and can make predictions, generate responses, or perform tasks based on this context.\n",
        "\n",
        "RNNs work in real time, constantly updating their understanding of the sequence as new inputs arrive. The hidden state at each time step is like a memory that adapts as the network processes more data.\n",
        "\n",
        "**Problems with RNNs**\n",
        "\n",
        "* Vanishing Gradient Problem:\n",
        "When RNNs are trained on long sequences, they struggle to learn long-term dependencies. This happens because, during the training process, the gradients (which are used to update the network’s parameters) can become extremely small as they are passed back through many layers or time steps. This makes it difficult for the network to remember earlier information, resulting in the vanishing gradient problem.\n",
        "* Short-Term Memory:\n",
        "Because of this problem, RNNs often only remember recent inputs well and struggle to capture long-term dependencies, limiting their effectiveness in tasks that require memory of distant information (like understanding long sentences or tracking long-term trends in time-series data).\n",
        "\n",
        "**LSTMs: A Solution to RNN Problems**\n",
        "To solve the limitations of RNNs, a variation called Long Short-Term Memory (LSTM) networks was developed. LSTMs introduce specialized components called memory cells and gates that help the network decide what to remember, what to update, and what to forget. These components allow LSTMs to capture long-term dependencies and solve the vanishing gradient problem.\n",
        "\n",
        "**How LSTMs Work (Simple Example):**\n",
        "In the sentence \"I went to the store, and then I bought...,\" an RNN might forget the context of \"store\" by the time it processes \"bought.\"\n",
        "In contrast, an LSTM can selectively remember important information like \"store\" and carry that memory forward through the sentence to make a more meaningful prediction like \"groceries\" after \"bought.\"\n",
        "The memory cell in an LSTM acts like a long-term memory that can hold onto important details over extended sequences, while gates control when to let new information in, when to forget, and when to pass information forward.\n",
        "\n",
        "**In Summary:**\n",
        "* RNNs are ideal for handling sequential data where the order of inputs matters, making them useful for tasks like language modeling, speech recognition, and time-series analysis.\n",
        "* RNNs process inputs in real time, constantly updating their hidden state as they receive new inputs.\n",
        "However, they suffer from memory limitations due to the vanishing gradient problem, which makes it difficult for them to retain information over long sequences.\n",
        "* LSTMs improve upon RNNs by using memory cells and gates to handle long-term dependencies, making them more effective at tasks requiring long-term memory."
      ],
      "metadata": {
        "id": "hk6micwe9122"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanishing Gradient Problem\n",
        "\n",
        "**What is the Vanishing Gradient Problem?**\n",
        "\n",
        "* The vanishing gradient problem happens when a neural network can't learn well because the updates or gradients become too small. This is common in deep networks like RNNs, where information passes through many layers or time steps. When gradients vanish, learning slows down or stops because the early layers don't get enough information to improve.\n",
        "\n",
        "**Why Does the Gradient Weaken?**\n",
        "\n",
        "* Multiplying Many Times: As gradients move backward through the layers, each one is multiplied by small numbers (weights and activation function derivatives). If these numbers are less than 1, the gradient becomes smaller and weaker with each step back.\n",
        "\n",
        "* Activation Functions: Functions like sigmoid or tanh also make the gradients smaller, especially when their input is far from zero, which further weakens the signal.\n",
        "\n",
        "**Why Wrec Affects This:**\n",
        "\n",
        "* Small Wrec: If the recurrent weight is too small, the signal gets weaker with each step, causing the gradient to vanish. The network forgets past information, making it hard to learn long-term patterns.\n",
        "\n",
        "* Big Wrec: If the recurrent weight is too large, the opposite happens. The gradients grow too fast, causing the network to take huge uncontrolled steps, making learning unstable.\n",
        "\n",
        "**Example (Convex Shape - Hill):**\n",
        "\n",
        "Imagine walking down a smooth hill (convex shape). The gradient tells you how big your steps should be.\n",
        "\n",
        "* Vanishing Gradient (Small Steps): If the gradient is too small, it’s like taking tiny steps down the hill. You’re still moving in the right direction, but it will take forever to reach the bottom (the optimal point).\n",
        "\n",
        "* Exploding Gradient (Big Steps): If the gradient is too large (big weights), it’s like taking huge leaps down the hill. You’ll move fast, but it’s easy to overshoot or fall, making it hard to control where you land. This causes learning to become chaotic.\n",
        "\n",
        "**In Short:**\n",
        "\n",
        "* Small weights = vanishing gradients (network can’t learn well, like taking tiny steps).\n",
        "\n",
        "* Big weights = exploding gradients (learning becomes unstable, like taking huge uncontrolled leaps).\n",
        "\n",
        "* The network needs balanced steps to learn effectively, avoiding both extremes."
      ],
      "metadata": {
        "id": "p2P0uHJG8vye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "PHG0ziNV-d7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTMs (Long Short-Term Memory)** are a type of neural network designed to handle sequential data and solve the short-term memory problem in RNNs. They are built to remember important information over long sequences and forget irrelevant details using a system of gates: the input gate, forget gate, and output gate.\n",
        "\n",
        "**Key Features:**\n",
        "* Long-term memory: LSTMs can remember information from earlier in the sequence, making them effective for tasks that need to capture long-range dependencies.\n",
        "* Gates control memory: These gates decide what to keep, what to forget, and how much of the memory should be used for making predictions.\n",
        "* Solves vanishing gradient problem: LSTMs avoid the issue of vanishing gradients in RNNs, which allows them to retain and learn from information over longer time steps.\n",
        "\n",
        "In short: LSTMs are an advanced version of RNNs that handle long sequences more effectively by controlling memory through gates, making them better at learning long-term patterns."
      ],
      "metadata": {
        "id": "C0wZ3CwBGdAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code description\n",
        "\n",
        "This code builds a Recurrent Neural Network (RNN) with LSTM layers to predict Google stock prices. First, it loads and preprocesses the data by selecting the \"Open\" column from the dataset. To ensure better performance, the stock prices are scaled between 0 and 1 using MinMaxScaler, which helps the neural network handle varying ranges of data more effectively.\n",
        "\n",
        "Sequences of 60 days of stock prices are created, where each sequence (input) will predict the stock price of the 61st day (output). This sequence-based approach allows the model to capture trends over time. The data is reshaped into a 3D format (number of samples, timesteps, 1 feature) since LSTM layers expect this structure for time-series data.\n",
        "\n",
        "The RNN is then constructed using four LSTM layers with dropout to prevent overfitting. The return_sequences=True parameter ensures the model returns the entire sequence of outputs for all but the last LSTM layer. The model is compiled using the Adam optimizer, which adjusts learning rates dynamically for better training, and it's trained over 100 epochs with a batch size of 32 to fit the input-output pairs.\n",
        "\n",
        "For testing, the test data (stock prices for 2017) is loaded and processed similarly. The last 60 days from the training data are combined with the test data to make predictions based on past prices. The test data is also scaled like the training data for consistency. The RNN then predicts stock prices for 2017.\n",
        "\n",
        "Finally, the real and predicted stock prices are plotted for comparison, allowing us to visually assess how well the model performed."
      ],
      "metadata": {
        "id": "lBEK1Tg4f2Op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Data Preprocessing\n"
      ],
      "metadata": {
        "id": "C1dHfOUD-h34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Importing necessary libraries for building the RNN\n",
        "# Libraries for data manipulation, visualization, and neural network creation.\n",
        "import numpy as np  # For numerical computations\n",
        "import matplotlib.pyplot as plt  # For plotting the results\n",
        "import pandas as pd  # For data loading and manipulation\n",
        "from sklearn.preprocessing import MinMaxScaler  # For scaling data to a specific range\n",
        "\n",
        "from keras.models import Sequential  # To create a sequential neural network\n",
        "from keras.layers import Dense  # For fully connected layers\n",
        "from keras.layers import LSTM  # For Long Short-Term Memory layers (recurrent layers)\n",
        "from keras.layers import Dropout  # To reduce overfitting by dropping neurons during training\n",
        "\n",
        "# Step 2: Loading and preprocessing the training set\n",
        "# Load the dataset for training, selecting the \"Open\" column of Google stock prices.\n",
        "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')\n",
        "training_set = dataset_train.iloc[:, 1:2].values  # Selecting only the 'Open' column\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "# Scaling the stock prices between 0 and 1 for better neural network performance.\n",
        "sc = MinMaxScaler(feature_range=(0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        "\n",
        "# Step 4: Creating a data structure with 60 timesteps and 1 output\n",
        "# We create sequences of 60 previous stock prices as input (X) and 1 future price as output (y).\n",
        "X_train = []  # Input sequences\n",
        "y_train = []  # Target prices\n",
        "for i in range(60, len(training_set_scaled)):\n",
        "    X_train.append(training_set_scaled[i-60:i, 0])  # Last 60 days' stock prices\n",
        "    y_train.append(training_set_scaled[i, 0])  # Stock price on the 61st day\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)  # Convert to NumPy arrays\n",
        "\n",
        "# Step 5: Reshaping the data for the LSTM\n",
        "# Reshape the input to 3D: (number of samples, number of timesteps, 1 feature).\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
      ],
      "metadata": {
        "id": "vyrm_zx8VYDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Building the RNN\n"
      ],
      "metadata": {
        "id": "hcWB_8_PUWWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Initializing the Recurrent Neural Network (RNN)\n",
        "# Using the Sequential model to stack layers one after another.\n",
        "rnn = Sequential()\n",
        "\n",
        "# Step 7: Adding the first LSTM layer with dropout regularization\n",
        "# LSTM with 50 units, returning sequences, with dropout to avoid overfitting.\n",
        "rnn.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
        "rnn.add(Dropout(0.2))  # Drops 20% of the neurons randomly during training\n",
        "\n",
        "# Step 8: Adding additional LSTM layers and dropout\n",
        "# Add 3 more LSTM layers with 50 units each, plus dropout after each layer.\n",
        "rnn.add(LSTM(units=50, return_sequences=True))\n",
        "rnn.add(Dropout(0.2))\n",
        "\n",
        "rnn.add(LSTM(units=50, return_sequences=True))\n",
        "rnn.add(Dropout(0.2))\n",
        "\n",
        "rnn.add(LSTM(units=50))\n",
        "rnn.add(Dropout(0.2))\n",
        "\n",
        "# Step 9: Adding the output layer\n",
        "# A Dense layer with 1 unit to predict the next stock price.\n",
        "rnn.add(Dense(units=1))\n",
        "\n",
        "# Step 10: Compiling the RNN\n",
        "# Compile the model with Adam optimizer and mean squared error (MSE) loss for regression.\n",
        "rnn.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Step 11: Fitting the RNN to the training set\n",
        "# Train the RNN with the training data, running 100 epochs and a batch size of 32.\n",
        "rnn.fit(X_train, y_train, epochs=100, batch_size=32)"
      ],
      "metadata": {
        "id": "dFfbsNis_YhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Making the predictions and visualising the results\n"
      ],
      "metadata": {
        "id": "Glqd7IHaVPke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Preparing the test set for prediction\n",
        "# Load the test data (actual stock prices for 2017).\n",
        "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
        "real_stock_price = dataset_test.iloc[:, 1:2].values  # Selecting only the 'Open' column\n",
        "\n",
        "# Step 13: Preparing inputs for prediction\n",
        "# Combine the training and test sets, and create input sequences of 60 timesteps for prediction.\n",
        "# 'dataset_train' contains the stock prices for training, and 'dataset_test' contains the stock prices for testing.\n",
        "# We concatenate these two datasets along the vertical axis (axis=0) to have a continuous time series of stock prices.\n",
        "# This allows us to pull the last 60 days of data from the training set to help predict the first day of the test set.\n",
        "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis=0)\n",
        "\n",
        "# Extract the relevant inputs from the combined dataset.\n",
        "# We want to take the last 60 days of data from the training set and combine them with the test set data.\n",
        "# len(dataset_total) gives the total number of rows in the combined dataset, and len(dataset_test) gives the number of rows in the test set.\n",
        "# We subtract 60 from the start to include the 60 timesteps before the test set period.\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values  # Last 60 + test data\n",
        "\n",
        "# Reshape the input data into a 2D array where each row corresponds to a stock price and there is 1 column (1 feature).\n",
        "# The '-1' argument tells NumPy to calculate the number of rows automatically, based on the length of 'inputs'.\n",
        "# The '1' specifies that we have 1 feature (the stock price) for each timestep.\n",
        "inputs = inputs.reshape(-1, 1)\n",
        "\n",
        "# Scale the inputs using the same MinMaxScaler ('sc') used for the training data to ensure consistent scaling.\n",
        "# This step is necessary because the model was trained on scaled data, so the test data must be scaled in the same way before predictions.\n",
        "inputs = sc.transform(inputs)  # Scale the inputs like we did with training data\n",
        "\n",
        "\n",
        "# Step 14: Creating input data for the test set\n",
        "# Similar to training, create sequences of 60 previous stock prices for the test set.\n",
        "# We need to create input sequences, where each sequence contains the stock prices of the previous 60 days to predict the next day.\n",
        "X_test = []\n",
        "\n",
        "# Loop through the test data to create input sequences.\n",
        "# The range(60, 80) means that we're creating sequences starting from the 60th day of the combined dataset up to the 80th day.\n",
        "# For each day 'i', we take the stock prices from 'i-60' to 'i' (the previous 60 days).\n",
        "for i in range(60, 80):\n",
        "    X_test.append(inputs[i-60:i, 0])  # Create sequences for the test set\n",
        "\n",
        "# Convert the test set input data into a NumPy array for use in the model.\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "# Reshape the input data to be 3D so it can be fed into the LSTM.\n",
        "# The shape is (number of samples, number of timesteps, 1 feature), where:\n",
        "# X_test.shape[0] = number of samples (days),\n",
        "# X_test.shape[1] = number of timesteps (60),\n",
        "# and 1 feature (the stock price at each timestep).\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Step 15: Predicting the stock prices for the test set\n",
        "# Use the trained model to predict future stock prices and reverse the scaling.\n",
        "predicted_stock_price = regressor.predict(X_test)\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
        "\n",
        "# Step 16: Visualizing the results\n",
        "# Plot the real vs predicted stock prices to compare the RNN's performance.\n",
        "plt.plot(real_stock_price, color='red', label='Real Google Stock Price')\n",
        "plt.plot(predicted_stock_price, color='blue', label='Predicted Google Stock Price')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KE0ZRsIM_wAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Use 2D Format:**\n",
        "* Many machine learning libraries expect input to be in a 2D shape, even when working with a single feature.\n",
        "\n",
        "**return_sequences=True in LSTM**\n",
        "* If return_sequences=True, the LSTM returns the output for all timesteps, which is passed to the next LSTM layer.\n",
        "If return_sequences=False, only the output from the last timestep is returned. In a stacked LSTM, return_sequences=True is typically used for the intermediate layers, so the following LSTM layers can get the entire sequence.\n",
        "\n",
        "**Timesteps in Stock Prediction**\n",
        "* Timesteps represent how many past data points the model looks at to make a prediction (e.g., using the stock prices from the last 60 days to predict tomorrow’s price).\n",
        "More timesteps mean the model remembers longer histories, like increasing the window from 60 days to 120 days.\n",
        "\n",
        "**Reshaping the Data for LSTM**\n",
        "* Reshaping converts the input data into a 3D format: (number of samples, number of timesteps, number of features).\n",
        "This is needed for LSTMs, which process data over time, like stock prices over the past 60 days.\n",
        "Example: You reshape data to (X_train.shape[0], 60, 1) where 60 is the number of timesteps and 1 is the single feature (stock price)."
      ],
      "metadata": {
        "id": "YULXrpxtWarP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving the RNN\n",
        "**Here are different ways to improve the RNN model:**\n",
        "\n",
        "* Getting more training data: we trained our model on the past 5 years of the Google Stock Price but it would be even better to train it on the past 10 years.\n",
        "\n",
        "* Increasing the number of timesteps: the model remembered the stock prices from the 60 previous financial days to predict the stock price of the next day. That’s because we chose a number of 60 timesteps (3 months). You could try to increase the number of timesteps, by choosing for example 120 timesteps (6 months).\n",
        "\n",
        "* Adding some other indicators: if you have the financial instinct that the stock price of some other companies might be correlated to the one of Google, you could add this other stock price as a new indicator in the training data.\n",
        "\n",
        "* Adding more LSTM layers: we built a RNN with four LSTM layers but you could try with even more.\n",
        "\n",
        "* Adding more neurones in the LSTM layers: we highlighted the fact that we needed a high number of neurones in the LSTM layers to respond better to the complexity of the problem and we chose to include 50 neurones in each of our 4 LSTM layers. You could try an architecture with even more neurones in each of the 4 (or more) LSTM layers."
      ],
      "metadata": {
        "id": "kBtkSArPB6z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Variations"
      ],
      "metadata": {
        "id": "g61nWzNdHa4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "jwpLYJx7HYyZ"
      }
    }
  ]
}